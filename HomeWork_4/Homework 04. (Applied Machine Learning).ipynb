{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the required python library for the homework below. Note that we have imported a python file(myFunction) that contains some functions we defined ourselves. Those functions will be explained in the appropriate palces before it is to be used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "import bokeh\n",
    "#####\n",
    "import myFunction # the function  has been defined by us\n",
    "####\n",
    "import seaborn as sns \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and  View the shape of data\n",
    "We imported the data and verify its shape  as afirst step in exploratory analysis of the data. We also provide a quick look of the first few rows of the data set. With this we get  a first hand visualization of the  data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape is: (146028, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerShort</th>\n",
       "      <th>player</th>\n",
       "      <th>club</th>\n",
       "      <th>leagueCountry</th>\n",
       "      <th>birthday</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>games</th>\n",
       "      <th>victories</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2</th>\n",
       "      <th>refNum</th>\n",
       "      <th>refCountry</th>\n",
       "      <th>Alpha_3</th>\n",
       "      <th>meanIAT</th>\n",
       "      <th>nIAT</th>\n",
       "      <th>seIAT</th>\n",
       "      <th>meanExp</th>\n",
       "      <th>nExp</th>\n",
       "      <th>seExp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lucas-wilchez</td>\n",
       "      <td>Lucas Wilchez</td>\n",
       "      <td>Real Zaragoza</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31.08.1983</td>\n",
       "      <td>177.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>Attacking Midfielder</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GRC</td>\n",
       "      <td>0.326391</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.002696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-utaka</td>\n",
       "      <td>John Utaka</td>\n",
       "      <td>Montpellier HSC</td>\n",
       "      <td>France</td>\n",
       "      <td>08.01.1982</td>\n",
       "      <td>179.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Right Winger</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>0.203375</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>-0.204082</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.061504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdon-prats</td>\n",
       "      <td>Abdón Prats</td>\n",
       "      <td>RCD Mallorca</td>\n",
       "      <td>Spain</td>\n",
       "      <td>17.12.1992</td>\n",
       "      <td>181.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pablo-mari</td>\n",
       "      <td>Pablo Marí</td>\n",
       "      <td>RCD Mallorca</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31.08.1993</td>\n",
       "      <td>191.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Center Back</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ruben-pena</td>\n",
       "      <td>Rubén Peña</td>\n",
       "      <td>Real Valladolid</td>\n",
       "      <td>Spain</td>\n",
       "      <td>18.07.1991</td>\n",
       "      <td>172.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Right Midfielder</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerShort         player             club leagueCountry    birthday  \\\n",
       "0  lucas-wilchez  Lucas Wilchez    Real Zaragoza         Spain  31.08.1983   \n",
       "1     john-utaka     John Utaka  Montpellier HSC        France  08.01.1982   \n",
       "2    abdon-prats    Abdón Prats     RCD Mallorca         Spain  17.12.1992   \n",
       "3     pablo-mari     Pablo Marí     RCD Mallorca         Spain  31.08.1993   \n",
       "4     ruben-pena     Rubén Peña  Real Valladolid         Spain  18.07.1991   \n",
       "\n",
       "   height  weight              position  games  victories    ...     rater2  \\\n",
       "0   177.0    72.0  Attacking Midfielder      1          0    ...       0.50   \n",
       "1   179.0    82.0          Right Winger      1          0    ...       0.75   \n",
       "2   181.0    79.0                   NaN      1          0    ...        NaN   \n",
       "3   191.0    87.0           Center Back      1          1    ...        NaN   \n",
       "4   172.0    70.0      Right Midfielder      1          1    ...        NaN   \n",
       "\n",
       "   refNum  refCountry  Alpha_3   meanIAT    nIAT     seIAT   meanExp    nExp  \\\n",
       "0       1           1      GRC  0.326391   712.0  0.000564  0.396000   750.0   \n",
       "1       2           2      ZMB  0.203375    40.0  0.010875 -0.204082    49.0   \n",
       "2       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "3       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "4       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "\n",
       "      seExp  \n",
       "0  0.002696  \n",
       "1  0.061504  \n",
       "2  0.001002  \n",
       "3  0.001002  \n",
       "4  0.001002  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawData_CrowdstormingDataJuly1st = pd.read_csv('CrowdstormingDataJuly1st.csv')\n",
    "print(\"Data Shape is: \"  + str(RawData_CrowdstormingDataJuly1st.shape)) # we use  str to convert tuple to string.\n",
    "RawData_CrowdstormingDataJuly1st.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Summary of Statistics About Data\n",
    "We provide a quick  summary of statistics about the data set. From this view we observe tha Nan is present in our data set and we have to do some cleaning. We also note that columns of  data type  object  (categorical variables)  were not reported here. We will deal with this as we go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>games</th>\n",
       "      <th>victories</th>\n",
       "      <th>ties</th>\n",
       "      <th>defeats</th>\n",
       "      <th>goals</th>\n",
       "      <th>yellowCards</th>\n",
       "      <th>yellowReds</th>\n",
       "      <th>redCards</th>\n",
       "      <th>rater1</th>\n",
       "      <th>rater2</th>\n",
       "      <th>refNum</th>\n",
       "      <th>refCountry</th>\n",
       "      <th>meanIAT</th>\n",
       "      <th>nIAT</th>\n",
       "      <th>seIAT</th>\n",
       "      <th>meanExp</th>\n",
       "      <th>nExp</th>\n",
       "      <th>seExp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>145765.000000</td>\n",
       "      <td>143785.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>124621.000000</td>\n",
       "      <td>124621.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>146028.000000</td>\n",
       "      <td>145865.000000</td>\n",
       "      <td>1.458650e+05</td>\n",
       "      <td>1.458650e+05</td>\n",
       "      <td>145865.000000</td>\n",
       "      <td>1.458650e+05</td>\n",
       "      <td>145865.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>181.935938</td>\n",
       "      <td>76.075662</td>\n",
       "      <td>2.921166</td>\n",
       "      <td>1.278344</td>\n",
       "      <td>0.708241</td>\n",
       "      <td>0.934581</td>\n",
       "      <td>0.338058</td>\n",
       "      <td>0.385364</td>\n",
       "      <td>0.011381</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.264255</td>\n",
       "      <td>0.302862</td>\n",
       "      <td>1534.827444</td>\n",
       "      <td>29.642842</td>\n",
       "      <td>0.346276</td>\n",
       "      <td>1.969741e+04</td>\n",
       "      <td>6.310849e-04</td>\n",
       "      <td>0.452026</td>\n",
       "      <td>2.044023e+04</td>\n",
       "      <td>0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.738726</td>\n",
       "      <td>7.140906</td>\n",
       "      <td>3.413633</td>\n",
       "      <td>1.790725</td>\n",
       "      <td>1.116793</td>\n",
       "      <td>1.383059</td>\n",
       "      <td>0.906481</td>\n",
       "      <td>0.795333</td>\n",
       "      <td>0.107931</td>\n",
       "      <td>0.112889</td>\n",
       "      <td>0.295382</td>\n",
       "      <td>0.293020</td>\n",
       "      <td>918.736625</td>\n",
       "      <td>27.496189</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>1.271262e+05</td>\n",
       "      <td>4.735857e-03</td>\n",
       "      <td>0.217469</td>\n",
       "      <td>1.306157e+05</td>\n",
       "      <td>0.019723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>161.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047254</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.235373e-07</td>\n",
       "      <td>-1.375000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1604.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2345.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3147.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.573793</td>\n",
       "      <td>1.975803e+06</td>\n",
       "      <td>2.862871e-01</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.029548e+06</td>\n",
       "      <td>1.060660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              height         weight          games      victories  \\\n",
       "count  145765.000000  143785.000000  146028.000000  146028.000000   \n",
       "mean      181.935938      76.075662       2.921166       1.278344   \n",
       "std         6.738726       7.140906       3.413633       1.790725   \n",
       "min       161.000000      54.000000       1.000000       0.000000   \n",
       "25%              NaN            NaN       1.000000       0.000000   \n",
       "50%              NaN            NaN       2.000000       1.000000   \n",
       "75%              NaN            NaN       3.000000       2.000000   \n",
       "max       203.000000     100.000000      47.000000      29.000000   \n",
       "\n",
       "                ties        defeats          goals    yellowCards  \\\n",
       "count  146028.000000  146028.000000  146028.000000  146028.000000   \n",
       "mean        0.708241       0.934581       0.338058       0.385364   \n",
       "std         1.116793       1.383059       0.906481       0.795333   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       1.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       0.000000       1.000000   \n",
       "max        14.000000      18.000000      23.000000      14.000000   \n",
       "\n",
       "          yellowReds       redCards         rater1         rater2  \\\n",
       "count  146028.000000  146028.000000  124621.000000  124621.000000   \n",
       "mean        0.011381       0.012559       0.264255       0.302862   \n",
       "std         0.107931       0.112889       0.295382       0.293020   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000            NaN            NaN   \n",
       "50%         0.000000       0.000000            NaN            NaN   \n",
       "75%         0.000000       0.000000            NaN            NaN   \n",
       "max         3.000000       2.000000       1.000000       1.000000   \n",
       "\n",
       "              refNum     refCountry        meanIAT          nIAT  \\\n",
       "count  146028.000000  146028.000000  145865.000000  1.458650e+05   \n",
       "mean     1534.827444      29.642842       0.346276  1.969741e+04   \n",
       "std       918.736625      27.496189       0.032246  1.271262e+05   \n",
       "min         1.000000       1.000000      -0.047254  2.000000e+00   \n",
       "25%       641.000000       7.000000            NaN           NaN   \n",
       "50%      1604.000000      21.000000            NaN           NaN   \n",
       "75%      2345.000000      44.000000            NaN           NaN   \n",
       "max      3147.000000     161.000000       0.573793  1.975803e+06   \n",
       "\n",
       "              seIAT        meanExp          nExp          seExp  \n",
       "count  1.458650e+05  145865.000000  1.458650e+05  145865.000000  \n",
       "mean   6.310849e-04       0.452026  2.044023e+04       0.002994  \n",
       "std    4.735857e-03       0.217469  1.306157e+05       0.019723  \n",
       "min    2.235373e-07      -1.375000  2.000000e+00       0.000001  \n",
       "25%             NaN            NaN           NaN            NaN  \n",
       "50%             NaN            NaN           NaN            NaN  \n",
       "75%             NaN            NaN           NaN            NaN  \n",
       "max    2.862871e-01       1.800000  2.029548e+06       1.060660  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawData_CrowdstormingDataJuly1st.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a copy of the Raw_CrowdstormingDataJuly1st as CrowdstormingDataJuly1st.\n",
    "With this we aim to leave the Raw_CrowdstormingDataJuly1st as default and continue preprocessing on CrowdstormingDataJuly1st. This division is mainly to enable us have one data set as default and the other as preprocessed one. This might be useful for comparison and analytical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next we make a copy of the data set. The new copy will be processed to remove Nan\n",
    "CrowdstormingDataJuly1st = RawData_CrowdstormingDataJuly1st.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with Nan we have to know where the Nan are in the data set.\n",
    "Below, we have a summary number of Nans present in each column. As we observe some columns have Nan while others don't. \n",
    "This step is important because it helps us us identify columns with Nan. Such columns will be filled with appropriate value to replace the NAn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerShort          0\n",
       "player               0\n",
       "club                 0\n",
       "leagueCountry        0\n",
       "birthday             0\n",
       "height             263\n",
       "weight            2243\n",
       "position         17726\n",
       "games                0\n",
       "victories            0\n",
       "ties                 0\n",
       "defeats              0\n",
       "goals                0\n",
       "yellowCards          0\n",
       "yellowReds           0\n",
       "redCards             0\n",
       "photoID          21407\n",
       "rater1           21407\n",
       "rater2           21407\n",
       "refNum               0\n",
       "refCountry           0\n",
       "Alpha_3              1\n",
       "meanIAT            163\n",
       "nIAT               163\n",
       "seIAT              163\n",
       "meanExp            163\n",
       "nExp               163\n",
       "seExp              163\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrowdstormingDataJuly1st.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now know which column has Nan. We need the data types of the column to be able to know with which values such Nan will be replaced. Below is the list of the columns present and their data types. In this observe int,float and column data types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerShort       object\n",
       "player            object\n",
       "club              object\n",
       "leagueCountry     object\n",
       "birthday          object\n",
       "height           float64\n",
       "weight           float64\n",
       "position          object\n",
       "games              int64\n",
       "victories          int64\n",
       "ties               int64\n",
       "defeats            int64\n",
       "goals              int64\n",
       "yellowCards        int64\n",
       "yellowReds         int64\n",
       "redCards           int64\n",
       "photoID           object\n",
       "rater1           float64\n",
       "rater2           float64\n",
       "refNum             int64\n",
       "refCountry         int64\n",
       "Alpha_3           object\n",
       "meanIAT          float64\n",
       "nIAT             float64\n",
       "seIAT            float64\n",
       "meanExp          float64\n",
       "nExp             float64\n",
       "seExp            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrowdstormingDataJuly1st.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns of Object data type having Nan in its values we replace the Nan with \"Unknown\" and convert such column to string data type. This is to facilitate encoding them. In particular converting to string  will enable  viewing multiple string  column values (\"Attacking MidFielder\") as one lexical word. This is important since the encoder-which we define later- now has to enode \"Attacking MidFielder\" as single semantic world but not \"Attacking\" and \"MidFielder\" as separate lexical entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We change the null value present in the three datatypes below to Unknown\n",
    "\n",
    "CrowdstormingDataJuly1st.position.fillna(\"Unknown\",inplace=True)\n",
    "CrowdstormingDataJuly1st.photoID.fillna(\"Unknown\",inplace=True)\n",
    "CrowdstormingDataJuly1st.Alpha_3.fillna(\"Unknown\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We cast the data types to string to enable using label encoder\n",
    "\n",
    "CrowdstormingDataJuly1st['position'] = CrowdstormingDataJuly1st['position'].astype('str') \n",
    "CrowdstormingDataJuly1st['photoID'] = CrowdstormingDataJuly1st['position'].astype('str') \n",
    "CrowdstormingDataJuly1st['Alpha_3'] = CrowdstormingDataJuly1st['position'].astype('str') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to  replace Nan present in rater1 and rater 2 with  a distinctive value -1. We think this is a better choice than averaging or using the median. This is because -1 becomes a separate value on it own. We can with that identify columns that are not identifiable rather than introducing bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill rater1 and rater2 with -1 where Nan occurs. \n",
    "#Zero Already appears in the rating so that using zero wont be a good choice\n",
    "\n",
    "CrowdstormingDataJuly1st.rater1.fillna(-1, inplace=True)\n",
    "CrowdstormingDataJuly1st.rater2.fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Variables   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other to use our data set with the RandomForest Classifier we have to encode the categorical variables present in the data set (These are the columns of data type object).Encoding basically gives each categorical value in the affected column(column with data type object) a unique identification number. This number is a reference to the object value it represent in the affected column. In this part of the of the homework, we provide provided step by step guide on how to do this. For Example \"Attacking MidFielder\" in the position column can be encoded as 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we collect all columns of object datatype from our data set below in CrowdstormingDataJuly1st_object_columns. The aim of this is be able to encode such columns at once. Sklearn as yet does not provide such functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CrowdstormingDataJuly1st_object_columns = CrowdstormingDataJuly1st.iloc[:, :].select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined  myFunction.MultiColumnLabelEncoder which  effectively Wraps sklearn LabelEncoder functionality(for encoding categorical variables) for use on multiple columns  of our data set. The multiple columns are those which were extracted above. As can be observed we parsed all such columns into myFunction.MultiColumnLabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an instance of MultiColumnLabelEncoder and Pass the columns extracted to it\n",
    "ColumnObjectEncoder = myFunction.MultiColumnLabelEncoder(columns=CrowdstormingDataJuly1st_object_columns )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The fit method of myFunction.MultiColumnLabelEncoder accesses individual column of our dataset via indexing. This effectively returns the columns of our data set which are of type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiColumnLabelEncoder(columns=Index(['playerShort', 'player', 'club', 'leagueCountry', 'birthday',\n",
       "       'position', 'photoID', 'Alpha_3'],\n",
       "      dtype='object'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ColumnObjectEncoder.fit(CrowdstormingDataJuly1st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the columns of  object data type to encoded labels. This we do by calling ColumnObjectEncoder.transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1185, 1341,   82, ...,    0,    0,    0],\n",
       "       [ 966, 1158,   62, ...,   11,   11,   11],\n",
       "       [   5,    0,   73, ...,   12,   12,   12],\n",
       "       ..., \n",
       "       [2007, 1982,   15, ...,    7,    7,    7],\n",
       "       [2020, 1992,  111, ...,    8,    8,    8],\n",
       "       [2039, 2011,    1, ...,    9,    9,    9]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally transform \n",
    "ColumnObjectEncoder.transform(CrowdstormingDataJuly1st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Data Preparation\n",
    "For the Random Forest Classifier we  convert our data set to array  and use one of rater1 and rater2 as our target variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We will like to know the unique values present in our target variable. As can be observed we introduced -1 which shows up below. In addition this will be multiclass classification since more than 2 values was returned. In addition rater1 and rater2 have same unique values so it might be safe to use one of the two as target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25  0.75 -1.    0.    1.    0.5 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5 ,  0.75, -1.  ,  0.  ,  0.25,  1.  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(CrowdstormingDataJuly1st.rater1.unique())\n",
    "CrowdstormingDataJuly1st.rater2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract rater1 and rater2 from the dataset and store them as an array called  TargetVAriableRater. We have to do this conversion to array to be able to use it as target variable in for Random forest classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146028, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TargetVAriableRater = CrowdstormingDataJuly1st[['rater1','rater2']]\n",
    "TargetVAriableRater = TargetVAriableRater.iloc[: , : ].values\n",
    "TargetVAriableRater.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the rater1 and rater2 from our remaining dataset since we have extrated it separately as shown above for use as target. Leaving the rater1 and rater2 in the data set will introduce some bias. In addition we output the shape of the remaining dataset.\n",
    "We store the dataset from which rater1 and rater2 hve been removed as an array called CrowdstormingDataJuly1st_Array. We have to do this conversion to array to be able to use in for Random forest classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146028, 26)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrowdstormingDataJuly1st.drop(['rater1','rater2'], axis=1,inplace='True')\n",
    "CrowdstormingDataJuly1st_Array = CrowdstormingDataJuly1st.iloc[: , : ].values\n",
    "CrowdstormingDataJuly1st_Array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  Split the Arrays generated from above  into train - test with 80 for train set and 20% for test. This enables us to train with the train sets and test with the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CrowdstormingDataJuly1st_Array,TargetVAriableRater , train_size=0.8, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets y _train and y_test contains rater1 and rater2. We seperate those into y_train_first/y_test_first for rater1 and y_train_second/y_test_second for rater 2. We will use one of the two in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_first = y_train[:,0]\n",
    "y_train_second = y_train[:,1]\n",
    "y_test_first = y_test[:,0]\n",
    "y_test_second = y_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we Import RandomForestClassifier and set an instance of it as RandomForestClassifier  . Note that the default uses Gini as  criterion in the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ForestClassifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other to use the y_tain in the ForestClassifier we have to convert to string type. This we have done below. We have also replaced all other Nan values present in X_train the with Zero. We replaced with zero so as to avoid inducing bias in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_first = np.asarray(y_train_first, dtype=\"|S6\")\n",
    "X_train[np.isnan(X_train)]=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our the ForestClassifier on the training sets ( y_train , x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForestClassifier.fit(X_train,y_train_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Predictions And Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the ForestClassifier lets save some predictions using the X_test set. But before we do that we convert all Nan present in X_test  to  zero as we did for the X_train. Note that y_test_pred is now the prediction our trianed model made when X_test was passed to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test[np.isnan(X_test)]=0\n",
    "y_test_pred= ForestClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have to test the accuracy of the prediction our model as made.This we do by comparing the y_test (from the data set) and the predicted values generated from X_test when it is parsed to the model. But we still need to convert the both y_test_first and y_test_pred to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99863041840717659"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_test_first = np.asarray(y_test_first, dtype=\"|S6\")\n",
    "#y_test_pred = np.asarray(y_test_first, dtype=\"|S6\")\n",
    "accuracy_score(y_test_first, y_test_pred , normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 0.99 accuracy score as above its obvious that the model is overfitting. Especially the test set is just too small so this explains the tendency for our model to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Part\n",
    "\n",
    "We import RamdomForest do feature extraction using our ForestClassifeier. We get as our output the columns of the data ser listed in order of importance. This is important so we can know which column actually plays a role in the classification process. We colected the column names in Feature_name and map them to their score as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.12839999999999999, 'player'), (0.12809999999999999, 'birthday'), (0.127, 'club'), (0.124, 'playerShort'), (0.11119999999999999, 'weight'), (0.1074, 'height'), (0.052299999999999999, 'photoID'), (0.049599999999999998, 'Alpha_3'), (0.049399999999999999, 'position'), (0.046800000000000001, 'leagueCountry'), (0.0146, 'meanIAT'), (0.0074999999999999997, 'meanExp'), (0.0066, 'nIAT'), (0.0066, 'nExp'), (0.0058999999999999999, 'seExp'), (0.0058999999999999999, 'refCountry'), (0.0054999999999999997, 'seIAT'), (0.0051999999999999998, 'games'), (0.0048999999999999998, 'refNum'), (0.0040000000000000001, 'victories'), (0.0025000000000000001, 'defeats'), (0.0023999999999999998, 'goals'), (0.002, 'ties'), (0.0018, 'yellowCards'), (0.00020000000000000001, 'redCards'), (0.0001, 'yellowReds')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Feature_name = CrowdstormingDataJuly1st.columns # Get Column Headings of our Dataframe\n",
    "\n",
    "print((sorted(zip(map(lambda value: round(value, 4), ForestClassifier.feature_importances_), Feature_name), \n",
    "             reverse=True))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coding below is just the same thing as above. The only thing that changed is that we use y_train_second (which  contains value for rater2)to train the classifier now. This is to show that both gives almost same result when it comes to importance of feature. Note that as usual we convert the y_train_second to string before parsing it to our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.13450000000000001, 'birthday'), (0.12790000000000001, 'player'), (0.1225, 'playerShort'), (0.121, 'club'), (0.11169999999999999, 'weight'), (0.1056, 'height'), (0.053400000000000003, 'position'), (0.051999999999999998, 'Alpha_3'), (0.050999999999999997, 'photoID'), (0.041700000000000001, 'leagueCountry'), (0.0147, 'meanIAT'), (0.0086, 'meanExp'), (0.0064999999999999997, 'refCountry'), (0.0063, 'nExp'), (0.0061000000000000004, 'nIAT'), (0.0060000000000000001, 'seIAT'), (0.0054000000000000003, 'seExp'), (0.0053, 'refNum'), (0.0053, 'games'), (0.0044000000000000003, 'victories'), (0.0027000000000000001, 'goals'), (0.0027000000000000001, 'defeats'), (0.0022000000000000001, 'ties'), (0.0019, 'yellowCards'), (0.00020000000000000001, 'yellowReds'), (0.00020000000000000001, 'redCards')]\n"
     ]
    }
   ],
   "source": [
    "y_train_second = np.asarray(y_train_second, dtype=\"|S6\")\n",
    "\n",
    "ForestClassifier.fit(X_train, y_train_second) \n",
    "\n",
    "print((sorted(zip(map(lambda x: round(x, 4), ForestClassifier.feature_importances_), \n",
    "                  Feature_name),reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RANDOMFOREST classifier and Effect of Parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters to adjust when using the RandomForest Classifier  is \"n_estimators\" and \"max_features\". \n",
    "\n",
    "The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition,  the results will stop getting significantly better beyond a critical number of trees. \n",
    "\n",
    "The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. \n",
    "\n",
    "Empirical good default values are  max_features=sqrt(n_features) for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we  generate various RandomForest classifier each having different parameters. The aim is to show how different parameters passed to the Classifier affect the overfitting issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ForestClassifierWithMaxFeatures = RandomForestClassifier(n_estimators=100,max_features = np.sqrt(len(Feature_name)) )\n",
    "ForestClassifierFewN_Estimators = RandomForestClassifier(n_estimators=10 )\n",
    "ForestClassifierFewN_Features = RandomForestClassifier(max_features=5)\n",
    "ForestClassifierFewTreesAndEstimators = RandomForestClassifier(n_estimators=5,max_features=5)\n",
    "ForestClassifierWithMoreEstimators = RandomForestClassifier(n_estimators=200,max_features = np.sqrt(len(Feature_name)) )\n",
    "ForestClassifierFewFullFeatures = RandomForestClassifier(n_estimators=200,max_features=len(Feature_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "    Use cross validation to give accuracy score of each of the models above\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
